"""
AI Service for Conversational Research Experience
Integrates Anthropic Claude for both conversational and deep research modes.
"""
import os
import json
import re
import time
from typing import List, Dict, Any, Optional
from datetime import datetime
import anthropic
from services.licensing.content_licensing import ContentLicenseService
from services.research.crawler import ContentCrawlerStub
# TierType removed - all reports are now Pro Package

class AIResearchService:
    """Unified AI service for conversational and deep research modes"""
    
    def __init__(self):
        self.client = anthropic.Anthropic(
            api_key=os.environ.get('ANTHROPIC_API_KEY')
        )
        self.license_service = ContentLicenseService()
        self.crawler = ContentCrawlerStub()
        # Legacy in-memory storage for backward compatibility
        # NOTE: This should be migrated to use database-backed conversation_history
        self.user_conversations = {}
    
    async def filter_search_results_by_relevance(self, query: str, results: List[Dict[str, Any]], publication: Optional[str] = None, conversation_context: Optional[List[Dict[str, Any]]] = None, enhanced_context: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Use Claude to filter search results for conversation-aware relevance.
        
        Args:
            query: User's research query
            results: List of search results from Tavily with 'title', 'content', 'url'
            publication: Optional specific publication name (e.g., "Wall Street Journal", "WSJ")
            conversation_context: Optional conversation history to evaluate relevance against
            enhanced_context: Optional enhanced_context from research brief extraction
        
        Returns:
            Filtered list of relevant results with reasoning
        """
        print(f"ðŸ”Ž Claude filtering STARTED - Query: '{query}', Results: {len(results)}, Publication: {publication}, Has Conversation: {conversation_context is not None}, Has Enhanced Context: {enhanced_context is not None}")
        
        if not results:
            print("âš ï¸  No results to filter, returning empty list")
            return []
        
        # Build conversation context for filtering
        conversation_summary = ""
        if conversation_context:
            # Build summary of last 8-10 messages
            recent_messages = conversation_context[-10:] if len(conversation_context) > 10 else conversation_context
            for msg in recent_messages:
                role = msg.get('sender', msg.get('role', 'user'))
                content = msg.get('content', '').strip()
                if content and len(content) > 5:
                    conversation_summary += f"{role.upper()}: {content}\n"
        
        # Build enhanced context summary
        context_details = ""
        if enhanced_context:
            geographic = enhanced_context.get("geographic_scope", "").strip()
            temporal = enhanced_context.get("temporal_scope", "").strip()
            aspects = enhanced_context.get("specific_aspects", [])
            exclusions = enhanced_context.get("exclusions", [])
            
            if geographic and geographic.lower() not in ["none", "global"]:
                context_details += f"\n- Geographic focus: {geographic}"
            if temporal and temporal.lower() != "none":
                context_details += f"\n- Time period: {temporal}"
            if aspects:
                context_details += f"\n- Specific aspects: {', '.join(aspects[:3])}"
            if exclusions:
                context_details += f"\n- EXCLUDE topics: {', '.join(exclusions)}"
        
        # Build evaluation prompt
        publication_context = ""
        if publication:
            publication_context = f"\n\nIMPORTANT: User specifically requested sources from '{publication}'. Heavily prioritize results from this publication and filter out results from other publications unless they are highly relevant to the core query."
        
        # Enhanced system prompt with conversation context
        if conversation_summary or context_details:
            system_prompt = f"""You are a research relevance expert. Your PRIMARY job is to ensure search results directly match what the user discussed in their conversation.

User's Research Query: "{query}"{context_details}

Conversation Context:
{conversation_summary if conversation_summary else "(No conversation history)"}

CRITICAL: Evaluate each result with MAXIMUM STRICTNESS against the conversation context:

1. **Conversation Alignment** (MOST IMPORTANT): Does this source directly address what the user discussed, asked about, or cared about in the conversation? If it's only tangentially related, mark as NOT relevant.

2. **Geographic/Temporal Match**: If the user specified geographic or time constraints in conversation, strictly enforce them. Sources that don't match = NOT relevant.

3. **Specific Aspects**: If the user asked about specific aspects/angles in the conversation, sources that don't cover those = NOT relevant.

4. **Exclusions** (HARD FILTER): If the user mentioned topics to EXCLUDE in conversation, immediately mark sources about those topics as NOT relevant.

5. **Topic Match**: Does the article genuinely discuss the EXACT core topic from conversation? General/overview articles about broader topics = NOT relevant unless conversation was general.

6. **Publication Focus**: If a specific publication was requested, strictly filter to ONLY that publication.

7. **Dead/Low-Quality Links**: If the title/summary suggests the page might be a 404, redirect, social media post, or low-quality content, mark as NOT relevant.

DEFAULT TO NOT RELEVANT: When in doubt, mark as NOT relevant. It's better to show 5 highly relevant sources than 20 mediocre ones.{publication_context}

For each result, respond with JSON:
{{"relevant": true/false, "reason": "brief explanation based on conversation context"}}"""
        else:
            # Fallback to query-only filtering with stricter default
            system_prompt = f"""You are a research relevance expert. Your job is to evaluate whether search results PRECISELY match a user's research query.

Consider these factors with MAXIMUM STRICTNESS:

1. **Topic Match**: Does the article DIRECTLY discuss the core topic? General overviews or tangential mentions = NOT relevant.

2. **Geographic Scope**: If query mentions a specific country/region, strictly filter to sources focused on that geography.

3. **Contextual Relevance**: Does the article directly answer or address the user's specific question/need? If not, NOT relevant.

4. **Tangential vs Core**: Immediately reject results that are only tangentially related or mention the topic in passing.

5. **Publication Focus**: If a specific publication was requested, strictly filter to ONLY that publication.

6. **Dead/Low-Quality Links**: If the title/summary suggests the page might be a 404, redirect, social media post, or low-quality content, mark as NOT relevant.

DEFAULT TO NOT RELEVANT: When in doubt, mark as NOT relevant. It's better to show fewer highly relevant sources than many mediocre ones.

User Query: "{query}"{publication_context}

For each result, respond with JSON:
{{"relevant": true/false, "reason": "brief explanation"}}"""

        try:
            # Prepare results summary for Claude
            results_text = "\n\n".join([
                f"Result {i+1}:\nTitle: {r.get('title', 'No title')}\nURL: {r.get('url', '')}\nSummary: {r.get('content', '')[:300]}..."
                for i, r in enumerate(results[:15])  # Limit to 15 to stay under token limits
            ])
            
            user_message = f"Evaluate these search results for relevance:\n\n{results_text}\n\nRespond with a JSON array of evaluations, one per result."
            
            print(f"ðŸ“¡ Calling Claude API for relevance filtering...")
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",  # High-quality filtering with current knowledge
                max_tokens=2000,
                temperature=0.0,  # Deterministic for consistency
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": user_message
                }]
            )
            print(f"âœ… Claude API response received")
            
            response_text = self._extract_response_text(response)
            
            # Parse Claude's evaluation
            try:
                # Extract JSON from response (handle markdown code blocks)
                import re
                json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
                if json_match:
                    evaluations = json.loads(json_match.group(1))
                else:
                    # Try parsing directly
                    evaluations = json.loads(response_text)
                
                # Filter results based on Claude's evaluation
                filtered_results = []
                filtered_reasons = []
                
                for i, (result, evaluation) in enumerate(zip(results, evaluations)):
                    if evaluation.get('relevant', False):
                        filtered_results.append(result)
                    else:
                        reason = evaluation.get('reason', 'Not relevant')
                        filtered_reasons.append(f"  â€¢ {result.get('title', 'Unknown')[:60]}... - {reason}")
                
                # Log filtering results
                if filtered_reasons:
                    print(f"ðŸ” Claude filtered out {len(filtered_reasons)} irrelevant results:")
                    for reason in filtered_reasons[:5]:  # Show first 5
                        print(reason)
                    if len(filtered_reasons) > 5:
                        print(f"  ... and {len(filtered_reasons) - 5} more")
                
                print(f"âœ… Claude relevance filtering: {len(results)} â†’ {len(filtered_results)} results")
                return filtered_results
                
            except json.JSONDecodeError as e:
                print(f"âš ï¸  Failed to parse Claude evaluation (using all results): {e}")
                print(f"Response was: {response_text[:200]}...")
                return results  # Return all results if parsing fails
                
        except Exception as e:
            print(f"âš ï¸  Claude filtering error (using all results): {e}")
            return results  # Fallback to all results on error
    
    async def optimize_search_query(self, raw_query: str, conversation_context: List[Dict[str, Any]], pinned_topic: str = None) -> str:
        """
        Use Claude to optimize a search query based on conversation context.
        
        Args:
            raw_query: User's raw query string
            conversation_context: List of conversation messages for context
            pinned_topic: Optional pinned research topic to anchor the query
        
        Returns:
            Optimized query string for Tavily search API
        """
        print(f"ðŸŽ¯ Query optimization STARTED - Raw query: '{raw_query}'")
        
        # Build conversation summary (last 6 messages)
        context_summary = ""
        if conversation_context:
            for msg in conversation_context[-6:]:
                # Frontend sends 'sender' field, not 'role'
                role = msg.get('sender', msg.get('role', 'unknown'))
                content = msg.get('content', '')[:200]  # Limit length
                context_summary += f"{role.upper()}: {content}\n"
        
        # Build system prompt with topic constraint if available
        if pinned_topic:
            system_prompt = f"""You are a query optimizer. Your job is to rewrite a user's search query for precision by incorporating relevant context from the conversation.

CRITICAL CONSTRAINT: The user is researching "{pinned_topic}". ALL queries must remain anchored to this topic. Do NOT change topics.

Hard rules (must obey):
1) Keep the topic "{pinned_topic}" in your optimized query. This is MANDATORY.
2) Treat all queries as refinements of "{pinned_topic}" (e.g., adding publication filters, requesting more sources).
3) Extract key entities and topics from the conversation context that are relevant to the user's query.
4) Do NOT add entities that are unrelated to both the query and conversation context.
5) Keep it 5â€“15 words. No punctuation unless needed for operators.
6) Remove filler words like "I want to understand", "really", "help me", "can we", "find", but preserve all substantive terms.

Examples with topic constraint:
- Topic: "renewable energy", Query: "anything from time magazine" â†’ Output: "renewable energy time magazine"
- Topic: "renewable energy", Query: "can we find paid sources?" â†’ Output: "renewable energy credible paid sources"
- Topic: "federal reserve", Query: "more from wsj" â†’ Output: "federal reserve WSJ"

Return ONLY the optimized query. No explanation."""
        else:
            system_prompt = """You are a query optimizer. Your job is to rewrite a user's search query for precision by incorporating relevant context from the conversation.

Hard rules (must obey):
1) When the user asks a follow-up question (e.g., "more sources about...", "what about...", "from publications like..."), incorporate the main topic from the conversation context.
2) Extract key entities and topics from the conversation context that are relevant to the user's query.
3) Do NOT add entities that are unrelated to both the query and conversation context.
4) Keep it 5â€“15 words. No punctuation unless needed for operators.
5) Remove filler words like "I want to understand", "really", "help me", "can we", "find", but preserve all substantive terms.

Examples with context:
- Context: "USER: federal reserve policy... ASSISTANT: sources about fed policy"
  Query: "can we find more sources from wsj and nyt?"
  Output: "federal reserve policy WSJ NYT economist"

- Context: "USER: climate change solutions... ASSISTANT: renewable energy sources"
  Query: "what about solar panels?"
  Output: "climate change solar panels"

- Query without context: "green energy" â†’ Output: "green energy"

Return ONLY the optimized query. No explanation."""

        try:
            user_message = f"""Conversation Context:
{context_summary}

User Query: "{raw_query}"

Generate an optimized search query (max 120 chars):"""

            print(f"ðŸ“¡ Calling Claude API for query optimization...")
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",  # Fast and cheap
                max_tokens=150,
                temperature=0.1,  # Low but not 0.0 - stable without brittleness
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": user_message
                }]
            )
            
            optimized_query = self._extract_response_text(response).strip()
            
            # Remove any quotes if Claude added them
            optimized_query = optimized_query.strip('"\'')
            
            # Safety: Fallback to raw query if Claude returns empty/whitespace
            if not optimized_query or optimized_query.isspace():
                print(f"âš ï¸  Claude returned empty query, using raw query")
                return raw_query
            
            # POST-GENERATION GUARD: Check if Claude introduced new entities
            if not self._validate_no_entity_injection(raw_query, optimized_query, context_summary):
                print(f"âš ï¸  Entity injection detected - reverting to raw query")
                return raw_query
            
            # Truncate to 120 chars if needed
            if len(optimized_query) > 120:
                optimized_query = optimized_query[:120].rsplit(' ', 1)[0]
            
            print(f"âœ… Query optimized: '{raw_query}' â†’ '{optimized_query}'")
            return optimized_query
            
        except Exception as e:
            print(f"âš ï¸  Query optimization failed (using raw query): {e}")
            return raw_query  # Fallback to raw query on error
    
    def _validate_no_entity_injection(self, raw_query: str, optimized_query: str, context_summary: str) -> bool:
        """
        Check if optimized query introduces new proper nouns/entities not in raw query or context.
        Returns True if valid (no injection), False if entities were added.
        
        Case-insensitive validation that supports:
        - Acronyms in any case (wsj, WSJ, Wsj all match)
        - Multi-word topics from context (federal reserve policy)
        - Context-aware follow-up queries
        """
        import re
        
        def extract_all_words(text: str) -> set:
            """Extract ALL words (case-insensitive) including potential entities and acronyms."""
            if not text:
                return set()
            # Get all words (2+ chars), normalize to lowercase
            all_words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
            return {w.lower() for w in all_words}
        
        def extract_entities(text: str) -> set:
            """Extract likely entities: proper nouns and all-caps acronyms."""
            if not text:
                return set()
            # Pattern 1: Capitalized words (proper nouns)
            proper_nouns = set(re.findall(r'\b[A-Z][a-zA-Z]{1,}\b', text))
            # Pattern 2: All-caps acronyms (2+ letters)
            acronyms = set(re.findall(r'\b[A-Z]{2,}\b', text))
            # Normalize to lowercase for case-insensitive comparison
            return {e.lower() for e in (proper_nouns | acronyms)}
        
        # Get ALL words from raw query (case-insensitive) - includes lowercase acronyms like "wsj"
        raw_words = extract_all_words(raw_query)
        
        # Get entities from context
        context_entities = extract_entities(context_summary)
        
        # Also extract ALL words from context (for multi-word topics)
        context_words = extract_all_words(context_summary)
        
        # Allowed words/entities = anything from raw query OR context
        allowed_words = raw_words | context_words | context_entities
        
        # Get entities from optimized query
        optimized_entities = extract_entities(optimized_query)
        
        # Check for introduced entities (not in allowed set)
        introduced = optimized_entities - allowed_words
        
        # Filter out common non-entity words that might be capitalized
        common_words = {'october', 'november', 'december', 'january', 'february', 'march', 
                       'april', 'may', 'june', 'july', 'august', 'september', 'monday', 
                       'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',
                       'today', 'yesterday', 'tomorrow', 'morning', 'evening', 'night',
                       # Geographic abbreviations and common pronouns
                       'us', 'uk', 'eu', 'usa', 'uae', 'ussr', 'un', 'nato', 'asean'}
        introduced = introduced - common_words
        
        if introduced:
            print(f"ðŸš¨ Entity injection detected: {introduced}")
            print(f"   Raw words: {raw_words}")
            print(f"   Context words (sampled): {list(context_words)[:10]}")
            print(f"   Optimized entities: {optimized_entities}")
            return False
        
        return True
    
    def _extract_response_text(self, response) -> str:
        """Safely extract text from Anthropic response."""
        try:
            if hasattr(response, 'content') and response.content and len(response.content) > 0:
                content_block = response.content[0]
                if hasattr(content_block, 'text') and content_block.text:
                    return content_block.text
                else:
                    return str(content_block)
            else:
                return str(response)
        except Exception:
            return str(response)
    
    def _should_suggest_research(self, user_id: str) -> tuple[bool, Optional[str]]:
        """
        Determine if we should suggest switching to research mode.
        Returns: (should_suggest: bool, topic_hint: Optional[str])
        """
        # Don't suggest if we already have
        if self.suggested_research.get(user_id, False):
            print(f"ðŸš« Research already suggested for user {user_id}, skipping")
            return False, None
        
        # Need at least 3 USER messages for context (meaning third exchange)
        user_history = self.user_conversations.get(user_id, [])
        user_messages = [msg for msg in user_history if msg["role"] == "user"]
        print(f"ðŸ“Š Checking research suggestion - User {user_id} has {len(user_messages)} user messages ({len(user_history)} total)")
        if len(user_messages) < 3:
            print(f"â³ Not enough user messages yet ({len(user_messages)} < 3), not suggesting research")
            return False, None
        
        # Build research brief from conversation
        try:
            from app.api.routes.research import _build_research_brief, _classify_intent_and_temporal
            
            # Get recent messages for context
            context_messages = user_history[-6:]
            last_user_message = next((msg["content"] for msg in reversed(user_history) if msg["role"] == "user"), "")
            
            # Extract research brief
            brief = _build_research_brief(context_messages, last_user_message)
            
            # Classify intent
            classification = _classify_intent_and_temporal(brief)
            intent = classification.get("intent", "general_research")
            
            # Research-worthy intents
            research_intents = ["news_event", "policy_analysis", "academic_causal", "business_trends", "historical_explainer"]
            
            if intent in research_intents:
                # Extract topic hint for prefill
                topic_hint = brief.get("topic") or (brief["entities"][0] if brief.get("entities") else None)
                return True, topic_hint
            
        except Exception as e:
            print(f"âš ï¸ Research suggestion detection error: {e}")
        
        return False, None
    
    def _detect_source_intent(self, user_message: str, user_id: str) -> Dict[str, Any]:
        """
        Detect if user is explicitly requesting source/research search.
        Returns: {needs_sources: bool, query: str, confidence: float}
        """
        # Get recent conversation context (last assistant + user turns)
        user_history = self.user_conversations.get(user_id, [])
        recent_messages = user_history[-4:] if len(user_history) >= 4 else user_history
        
        # Build context for Claude
        context = "\n".join([
            f"{msg['role'].upper()}: {msg['content'][:200]}"
            for msg in recent_messages
        ])
        
        system_prompt = """You are an intent classifier. Analyze if the user is explicitly requesting to search for sources, articles, or research.

EXPLICIT source requests include:
- "find sources on this"
- "search for sources about X"
- "show me sources/articles/research"
- "I'd like to see sources on X"
- "get me sources/articles about this"
- "that's interesting, find sources on this aspect"
- Asking about specific publications: "what does WSJ say about X", "any articles from NYT on X", "Wall Street Journal coverage of X"
- Asking about recent events or topics that require current sources: "what happened with X", "recent developments in X"
- Natural variations expressing intent to search for authoritative content

DO NOT flag as source requests:
- Pure conceptual questions: "what is X", "how does Y work"
- Asking for explanations or opinions without needing sources
- Casual conversation about capabilities

Respond with ONLY valid JSON (no markdown):
{"needs_sources": true/false, "query": "extracted search query", "confidence": 0.0-1.0}

If needs_sources is true, extract a clear search query from the context. Include publication names if mentioned (e.g., "Wall Street Journal")."""

        try:
            # Use Claude to detect intent
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",  # Fast and cheap
                max_tokens=200,
                temperature=0.0,  # Deterministic
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": f"Recent conversation:\n{context}\n\nLatest message: {user_message}\n\nDoes the user want to search for sources?"
                }]
            )
            
            response_text = self._extract_response_text(response).strip()
            
            # Parse JSON response
            try:
                result = json.loads(response_text)
                needs_sources = result.get("needs_sources", False)
                query = result.get("query", user_message)
                confidence = result.get("confidence", 0.0)
                
                if needs_sources:
                    print(f"ðŸ” Intent Detection: Source search requested (confidence: {confidence:.2f})")
                    print(f"   Extracted query: {query}")
                
                return {
                    "needs_sources": needs_sources,
                    "query": query,
                    "confidence": confidence
                }
                
            except json.JSONDecodeError:
                print(f"âš ï¸ Intent detection JSON parse failed: {response_text}")
                # Default to no intent detected
                return {"needs_sources": False, "query": user_message, "confidence": 0.0}
                
        except Exception as e:
            print(f"âš ï¸ Intent detection error: {e}")
            # Default to no intent detected
            return {"needs_sources": False, "query": user_message, "confidence": 0.0}
        
    
    async def chat_with_context(
        self, 
        user_message: str, 
        mode: str = "conversational", 
        user_id: str = "anonymous",
        conversation_history: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Main chat interface supporting both conversational and deep research modes.
        Uses provided conversation history instead of in-memory storage.
        
        Args:
            user_message: The user's message
            mode: Chat mode ("conversational" or "deep_research")
            user_id: User identifier
            conversation_history: List of previous messages from database
            
        Returns:
            Response dictionary with AI response and metadata
        """
        # Use provided history or empty list
        if conversation_history is None:
            conversation_history = []
        
        if mode == "chat" or mode == "conversational":
            # Detect if user is explicitly requesting sources
            intent_result = self._detect_source_intent(user_message, user_id)
            return self._conversational_response_with_context(
                user_message, 
                user_id, 
                conversation_history,
                intent_result
            )
        else:  # research or deep_research
            return await self._deep_research_response_with_context(
                user_message, 
                user_id,
                conversation_history
            )
    
    def _conversational_response_with_context(
        self, 
        user_message: str, 
        user_id: str, 
        conversation_history: List[Dict[str, Any]],
        intent_result: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """Generate conversational response using provided conversation history"""
        
        if intent_result is None:
            intent_result = {"needs_sources": False, "query": "", "confidence": 0.0}
        
        current_date = datetime.now().strftime("%B %d, %Y")
        
        system_prompt = f"""You are an expert research guidance assistant helping users refine their research process through thoughtful conversation.

IMPORTANT CONTEXT:
- Today's date is {current_date}
- You HAVE ACCESS to current articles and sources through our integrated search system
- You can search for and access articles from major publications like WSJ, NYT, Forbes, and more
- Focus on helping users find the information they need rather than discussing limitations

Your role is to guide users toward precise, well-scoped research:

1. **Guide Scope Refinement**: Ask clarifying questions to help users narrow or expand their research focus
   - Geographic scope: "Are you interested in this globally, or focused on a specific region?"
   - Temporal scope: "Do you want recent developments, or historical context?"
   - Source preferences: "Would academic studies be helpful, or are you looking for journalism/policy analysis?"

2. **Identify Constraints**: Help users articulate what they DO and DON'T want
   - "What specific aspects are most important to you?"
   - "Is there anything you'd like to exclude from your research?"

3. **Leverage Source Search**: When users ask about specific topics or publications, acknowledge you can search for sources
   - "I can search for recent articles on that topic from credible sources"
   - "I can find articles from the Wall Street Journal, NYT, and other major publications on this topic"
   - "Let me search for authoritative sources on this topic for you"

4. **Emphasize Credibility**: Frame research as finding the most trustworthy sources
   - "I'll help you find the most credible sources on this topicâ€”whether they're free or paywalled"
   - "Premium sources from major publications and peer-reviewed journals often provide the most authoritative analysis"

5. **Encourage Specificity**: Ask follow-up questions to understand their real information needs
   - Do not accept vague queriesâ€”help them get specific about what they are trying to learn

Be curious but not overbearing. Guide naturally through conversation, not interrogation.
When users ask about specific topics or publications, let them know you can search for sources right away.
Only mention knowledge limitations if absolutely necessaryâ€”focus on capabilities, not limitations."""
        
        # Convert provided history to Claude message format
        messages = []
        for msg in conversation_history[-10:]:  # Last 10 messages for context
            role = msg.get('sender', 'user')
            # Claude API expects 'user' or 'assistant', not 'system'
            if role == 'system':
                role = 'assistant'
            messages.append({
                "role": role,
                "content": msg['content']
            })
        
        # Add current message
        messages.append({
            "role": "user",
            "content": user_message
        })
        
        try:
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=1000,
                temperature=0.7,
                system=system_prompt,
                messages=messages
            )
            
            ai_response = self._extract_response_text(response)
            
            # Check if we should suggest switching to research mode
            # Use conversation history length instead of in-memory
            should_suggest = len(conversation_history) >= 3 and user_id not in self.suggested_research
            topic_hint = None
            
            result = {
                "response": ai_response,
                "mode": "conversational",
                "conversation_length": len(conversation_history) + 2,  # +2 for current exchange
                "suggest_research": should_suggest,
                # Intent detection fields
                "source_search_requested": intent_result.get("needs_sources", False),
                "source_query": intent_result.get("query", ""),
                "source_confidence": intent_result.get("confidence", 0.0)
            }
            
            # Mark that we've suggested for this user
            if should_suggest:
                self.suggested_research[user_id] = True
                print(f"ðŸ’¡ Suggesting research mode switch{f' for topic: {topic_hint}' if topic_hint else ''}")
                
                if topic_hint:
                    result["topic_hint"] = topic_hint
            
            return result
            
        except Exception as e:
            return {
                "response": f"I'm having trouble connecting right now. Let me help you explore your research interests anyway - what specific aspect of your topic are you most curious about?",
                "mode": "conversational", 
                "conversation_length": len(conversation_history) + 1,
                "suggest_research": False,
                "source_search_requested": False,
                "source_query": "",
                "source_confidence": 0.0,
                "error": str(e)
            }
    
    async def _deep_research_response_with_context(
        self, 
        user_message: str, 
        user_id: str,
        conversation_history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Generate deep research with context-aware source selection using provided history"""
        
        # Extract research context from provided history
        conversation_context = self._extract_research_context_from_history(conversation_history)
        
        system_prompt = f"""You are an expert research analyst. Based on this conversation history about the user's research interests:

{conversation_context}

The user is now requesting deep research. Your task is to:
1. Generate a comprehensive research query that captures their specific interests from our conversation
2. Create relevant search terms that will find the most valuable sources
3. Focus on finding both free and premium licensed sources that directly address their research goals

Be specific and targeted based on our conversation. Don't be generic."""

        try:
            # Get research strategy from Claude
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=500,
                temperature=0.3,
                system=system_prompt,
                messages=[{"role": "user", "content": f"Generate a targeted research query for: {user_message}"}]
            )
            
            research_query = self._extract_response_text(response).strip()
            
            # Execute deep research with the refined query
            return await self._execute_deep_research(research_query, user_message, user_id)
            
        except Exception as e:
            print(f"âš ï¸ Research context extraction failed: {e}")
            # Fallback: use original user message as query
            return await self._execute_deep_research(user_message, user_message, user_id)
    
    def _extract_research_context_from_history(self, conversation_history: List[Dict[str, Any]]) -> str:
        """Extract key research themes from provided conversation history"""
        
        # Debug logging
        print(f"ðŸ” Extracting context from {len(conversation_history)} messages")
        
        # Include both user and assistant messages for richer context
        recent_messages = [
            msg["content"] for msg in conversation_history[-8:] 
            if msg.get("sender") in ["user", "assistant"] and len(msg.get("content", "").strip()) > 10
        ]
        
        # Debug: show message preview
        print(f"ðŸ“ Message preview: {[m[:80] + '...' if len(m) > 80 else m for m in recent_messages[:3]]}")
        
        # De-duplicate similar messages
        seen = set()
        unique_messages = []
        for msg in recent_messages:
            # Simple dedup based on first 50 chars
            msg_key = msg[:50].lower()
            if msg_key not in seen:
                seen.add(msg_key)
                unique_messages.append(msg)
        
        if not unique_messages:
            return "No previous conversation context available."
        
        context_summary = "\n".join([f"- {msg}" for msg in unique_messages])
        return f"Conversation history:\n{context_summary}"
    
    async def _deep_research_response(self, user_message: str, user_id: str) -> Dict[str, Any]:
        """Generate deep research with context-aware source selection"""
        
        # Analyze conversation history to understand research focus
        conversation_context = self._extract_research_context(user_id)
        
        system_prompt = f"""You are an expert research analyst. Based on this conversation history about the user's research interests:

{conversation_context}

The user is now requesting deep research. Your task is to:
1. Generate a comprehensive research query that captures their specific interests from our conversation
2. Create relevant search terms that will find the most valuable sources
3. Focus on finding both free and premium licensed sources that directly address their research goals

Be specific and targeted based on our conversation. Don't be generic."""

        try:
            # Get research strategy from Claude
            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=500,
                temperature=0.3,
                system=system_prompt,
                messages=[{"role": "user", "content": f"Generate a targeted research query for: {user_message}"}]
            )
            
            research_query = self._extract_response_text(response).strip()
            
            # Execute deep research with the refined query
            return await self._execute_deep_research(research_query, user_message, user_id)
            
        except Exception as e:
            print(f"âš ï¸ Research context extraction failed: {e}")
            # Fallback: use original user message as query
            return await self._execute_deep_research(user_message, user_message, user_id)
    
    def _extract_research_context(self, user_id: str) -> str:
        """Extract key research themes from conversation history"""
        user_history = self.user_conversations.get(user_id, [])
        
        # Debug logging
        print(f"ðŸ” Extracting context for user_id={user_id}, found {len(user_history)} messages")
        
        # Include both user and assistant messages for richer context
        recent_messages = [
            msg["content"] for msg in user_history[-8:] 
            if msg["role"] in ["user", "assistant"] and len(msg["content"].strip()) > 10
        ]
        
        # Debug: show message preview
        print(f"ðŸ“ Message preview: {[m[:80] + '...' if len(m) > 80 else m for m in recent_messages[:3]]}")
        
        # De-duplicate similar messages
        seen = set()
        unique_messages = []
        for msg in recent_messages:
            # Simple deduplication by content similarity
            msg_clean = msg.strip().lower()
            if msg_clean not in seen and len(msg_clean) > 10:
                unique_messages.append(msg)
                seen.add(msg_clean)
        
        print(f"âœ… Final unique messages: {len(unique_messages)}")
        return "\n".join([f"- {msg}" for msg in unique_messages])
    
    async def _execute_deep_research(self, refined_query: str, original_query: str, user_id: str) -> Dict[str, Any]:
        """Execute deep research with intelligent source selection"""
        
        try:
            # Generate sources using the refined query
            sources = await self.crawler.generate_sources(refined_query, 15)  # Get more to select from
            
            # Convert to dicts and discover licensing
            sources_dicts = []
            for source in sources:
                source_dict = source.dict()
                
                # Check for licensing
                if source.licensing_protocol:
                    source_dict['license_info'] = {
                        'protocol': source.licensing_protocol,
                        'terms': {
                            'protocol': source.licensing_protocol,
                            'ai_include_price': source.license_cost,
                            'publisher': source.publisher_name
                        }
                    }
                
                sources_dicts.append(source_dict)
            
            # Intelligently select the 10 most relevant sources
            selected_sources = self._select_relevant_sources(sources_dicts, refined_query, 10)
            
            # Calculate dynamic pricing based on selected sources
            licensing_summary = self.license_service.get_license_summary(selected_sources)
            
            # Generate AI-powered research outline
            outline = self._generate_research_outline(selected_sources, refined_query)
            
            # Create the research response
            ai_response = f"Based on our conversation, I've found {len(selected_sources)} highly relevant sources for your research on this topic. Here's what I discovered:\n\n{outline}"
            
            # Add to user-specific conversation history
            if user_id not in self.user_conversations:
                self.user_conversations[user_id] = []
            
            self.user_conversations[user_id].append({
                "role": "assistant",
                "content": ai_response,
                "timestamp": datetime.now().isoformat(),
                "mode": "research"
            })
            
            return {
                "response": ai_response,
                "mode": "research",
                "sources": selected_sources,
                "licensing_summary": licensing_summary,
                "refined_query": refined_query,
                "total_cost": licensing_summary.get('total_cost', 0.0),
                "conversation_length": len(self.user_conversations.get(user_id, []))
            }
            
        except Exception as e:
            return {
                "response": f"I encountered an issue during research, but I can still help you explore this topic conversationally. What specific aspect would you like to discuss?",
                "mode": "research",
                "error": str(e)
            }
    
    def _select_relevant_sources(self, all_sources: List[Dict], query: str, count: int) -> List[Dict]:
        """Use AI to select the most relevant sources based on conversation context"""
        
        try:
            # Create source summaries for Claude to evaluate
            source_summaries = []
            for i, source in enumerate(all_sources[:20]):  # Limit to first 20 for efficiency
                summary = f"{i}: {source['title']} - {source['excerpt'][:200]}..."
                if source.get('license_info'):
                    summary += f" [Licensed: {source['license_info']['terms']['protocol']}]"
                source_summaries.append(summary)
            
            prompt = f"""Based on this research query: "{query}"

And these available sources:
{chr(10).join(source_summaries)}

Select the {count} most relevant and valuable source numbers (0-{len(source_summaries)-1}). 
Prioritize sources that directly address the research question, come from authoritative publishers, and provide unique insights.
Include a mix of free and licensed sources when licensed sources offer significantly higher value.

Return only the numbers separated by commas, like: 0,3,7,12"""

            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=100,
                temperature=0.1,
                messages=[{"role": "user", "content": prompt}]
            )
            
            # Parse selected indices
            selected_indices = [int(x.strip()) for x in self._extract_response_text(response).strip().split(',') if x.strip().isdigit()]
            
            # Return selected sources
            return [all_sources[i] for i in selected_indices if i < len(all_sources)][:count]
            
        except Exception as e:
            # Fallback: return first N sources
            return all_sources[:count]
    
    def _generate_research_outline(self, sources: List[Dict], query: str) -> str:
        """Generate AI-powered research outline based on selected sources"""
        
        try:
            source_summaries = [
                f"â€¢ {source['title']} ({source['domain']})"
                for source in sources
            ]
            
            prompt = f"""Based on this research query: "{query}"

And these selected sources:
{chr(10).join(source_summaries)}

Create a compelling research outline that shows:
1. Key themes and insights you can explore
2. What unique perspectives these sources offer
3. How they connect to answer the research question
4. The most interesting findings or trends

Keep it concise but compelling - make the user excited about what they'll discover."""

            response = self.client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=400,
                temperature=0.6,
                messages=[{"role": "user", "content": prompt}]
            )
            
            return self._extract_response_text(response)
            
        except Exception:
            return "Here are the most relevant sources I found for your research. Each offers unique insights that will help answer your questions."